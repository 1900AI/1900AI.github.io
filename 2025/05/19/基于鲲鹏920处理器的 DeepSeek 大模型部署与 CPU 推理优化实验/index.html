<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>1900</title>
    <link rel="stylesheet" href="/css/main.css">
<meta name="generator" content="Hexo 7.3.0"><style>mjx-container[jax="SVG"] {
  direction: ltr;
}

mjx-container[jax="SVG"] > svg {
  overflow: visible;
}

mjx-container[jax="SVG"][display="true"] {
  display: block;
  text-align: center;
  margin: 1em 0;
}

mjx-container[jax="SVG"][justify="left"] {
  text-align: left;
}

mjx-container[jax="SVG"][justify="right"] {
  text-align: right;
}

g[data-mml-node="merror"] > g {
  fill: red;
  stroke: red;
}

g[data-mml-node="merror"] > rect[data-background] {
  fill: yellow;
  stroke: none;
}

g[data-mml-node="mtable"] > line[data-line] {
  stroke-width: 70px;
  fill: none;
}

g[data-mml-node="mtable"] > rect[data-frame] {
  stroke-width: 70px;
  fill: none;
}

g[data-mml-node="mtable"] > .mjx-dashed {
  stroke-dasharray: 140;
}

g[data-mml-node="mtable"] > .mjx-dotted {
  stroke-linecap: round;
  stroke-dasharray: 0,140;
}

g[data-mml-node="mtable"] > svg {
  overflow: visible;
}

[jax="SVG"] mjx-tool {
  display: inline-block;
  position: relative;
  width: 0;
  height: 0;
}

[jax="SVG"] mjx-tool > mjx-tip {
  position: absolute;
  top: 0;
  left: 0;
}

mjx-tool > mjx-tip {
  display: inline-block;
  padding: .2em;
  border: 1px solid #888;
  font-size: 70%;
  background-color: #F8F8F8;
  color: black;
  box-shadow: 2px 2px 5px #AAAAAA;
}

g[data-mml-node="maction"][data-toggle] {
  cursor: pointer;
}

mjx-status {
  display: block;
  position: fixed;
  left: 1em;
  bottom: 1em;
  min-width: 25%;
  padding: .2em .4em;
  border: 1px solid #888;
  font-size: 90%;
  background-color: #F8F8F8;
  color: black;
}

foreignObject[data-mjx-xml] {
  font-family: initial;
  line-height: normal;
  overflow: visible;
}

.MathJax path {
  stroke-width: 3;
}

mjx-container[display="true"] {
  overflow: auto hidden;
}

mjx-container[display="true"] + br {
  display: none;
}
</style><link rel="alternate" href="/atom.xml" title="1900" type="application/atom+xml">
</head>
<body>
    <header>
        <div class="header-content">
            <div class="welcome">你好 👋 我是1900</div>
            <nav>
                <a href="/">主页</a>
                <a href="/categories">文章</a>
                <a href="/friends">友人</a>
                <a href="/about">关于</a>
                <a href="/atom.xml" target="_blank" title="RSS订阅">
                    <i class="czs-rss"></i>RSS
                </a>
            </nav>
        </div>
    </header>

    <main>
        <div class="post-with-toc">
    <!-- 左侧目录栏 -->
    
    <div class="sidebar-toc">
        <div class="toc-wrap">
            <div class="toc-header">
                <i class="czs-menu-l"></i>
                <span class="toc-title">目录</span>
            </div>
            <div class="toc-content">
                <ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%AE%9E%E9%AA%8C%E7%9B%AE%E7%9A%84%E4%B8%8E%E8%83%8C%E6%99%AF"><span class="toc-text">实验目的与背景</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%AE%9E%E9%AA%8C%E8%AE%BE%E5%A4%87"><span class="toc-text">实验设备</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%AE%9E%E9%AA%8C%E5%8E%9F%E7%90%86"><span class="toc-text">实验原理</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%AE%9E%E8%B7%B5%E8%BF%87%E7%A8%8B%E4%B8%8E%E7%BB%93%E6%9E%9C"><span class="toc-text">实践过程与结果</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%85%B6%E4%BB%96%E5%B0%8F%E7%BB%84%E9%A2%98%E7%9B%AE"><span class="toc-text">其他小组题目</span></a></li></ol>
            </div>
        </div>
    </div>
    

    <!-- 文章主体内容 -->
    <article class="post">
        <div class="post-container">
            <h1 class="post-title">基于鲲鹏920处理器的 DeepSeek 大模型部署与 CPU 推理优化实验</h1>
            <div class="post-meta">
                <span class="post-date">
                    <i class="czs-calendar"></i>
                    2025-05-19
                </span>
                
                    <span class="post-categories">
                        <i class="czs-folder"></i>
                        <a class="post-category-link" href="/categories/%E7%AC%94%E8%AE%B0/">笔记</a>
                    </span>
                
                
                    <span class="post-tags">
                        <i class="czs-tag"></i>
                        <a class="post-tag-none-link" href="/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BB%84%E6%88%90%E5%8E%9F%E7%90%86%E5%A4%A7%E4%BD%9C%E4%B8%9A/" rel="tag">计算机组成原理大作业</a>
                    </span>
                
                <span class="post-visitors">
                    <i class="czs-eye"></i>
                    <span id="/2025/05/19/%E5%9F%BA%E4%BA%8E%E9%B2%B2%E9%B9%8F920%E5%A4%84%E7%90%86%E5%99%A8%E7%9A%84%20DeepSeek%20%E5%A4%A7%E6%A8%A1%E5%9E%8B%E9%83%A8%E7%BD%B2%E4%B8%8E%20CPU%20%E6%8E%A8%E7%90%86%E4%BC%98%E5%8C%96%E5%AE%9E%E9%AA%8C/" class="leancloud-visitors" data-flag-title="基于鲲鹏920处理器的 DeepSeek 大模型部署与 CPU 推理优化实验">
                        <span class="leancloud-visitors-count">0</span> 次浏览
                    </span>
                </span>
            </div>
            
            <div class="post-content">
                <h2 id="实验目的与背景">实验目的与背景</h2>
<p>本实验在华为鲲鹏920 ARM多核服务器上，使用 Ollama 框架部署 DeepSeek
大语言模型，探索<strong>无 GPU
环境下的推理性能优化方案</strong>。通过多核并行、ARM NEON SIMD 加速和
INT8/INT4
等低精度量化技术，分析各类优化对推理效率和资源占用的影响，进而掌握 CPU
平台下大模型部署的核心策略。</p>
<h2 id="实验设备">实验设备</h2>
<ul>
<li><strong>服务器</strong>：华为鲲鹏云服务器（KM1 型号，48 核 384 GiB
内存）</li>
<li><strong>终端设备</strong>：具备网络连接的个人电脑</li>
<li><strong>操作系统</strong>：openEuler 20.03 LTS SP1（ARM64）</li>
</ul>
<h2 id="实验原理">实验原理</h2>
<p>在纯 CPU 环境中部署大型语言模型（LLM）是一项极具挑战的任务。由于 LLM
拥有庞大的参数量、复杂的网络结构以及高度计算密集的特性，其推理过程对计算能力和内存带宽提出了极高要求。与
GPU 不同，传统 CPU 缺乏用于加速大规模张量计算的专用单元（如 Tensor
Core），因此在 CPU 上运行 LLM 通常依赖于多核并行、SIMD
向量化及高效的内存调度等软硬件协同优化手段。</p>
<p>本实验基于鲲鹏920服务器平台，结合 DeepSeek 模型与 Ollama
推理框架，从硬件架构理解、推理瓶颈识别到多核调度与 NUMA
优化、量化模型，逐步分析 CPU 部署 LLM 的关键技术路径。</p>
<h3 id="鲲鹏920-架构特性与推理瓶颈分析">鲲鹏920
架构特性与推理瓶颈分析</h3>
<p>鲲鹏920 是基于 ARM 架构的高性能服务器芯片，具备如下硬件优势：</p>
<ul>
<li>大量物理核心，支持高并行度计算；</li>
<li>高内存带宽，适合频繁数据读写；</li>
<li>支持 SIMD（如 NEON）指令集，提升向量运算效率。</li>
</ul>
<p>尽管具备上述优势，CPU 运行 LLM 推理仍面临如下挑战：</p>
<ul>
<li><strong>吞吐量低</strong>：推理过程为逐 token
生成，难以批量处理，限制整体生成速率；</li>
<li><strong>内存访问频繁</strong>：模型涉及大量权重加载与缓存读写，对内存带宽和缓存命中率要求极高；</li>
<li><strong>资源利用率不均衡</strong>：若线程调度不合理，或频繁跨 NUMA
节点访问内存，容易造成带宽冲突、缓存失衡，降低整体性能。</li>
</ul>
<h3 id="多核调度与-numa-优化机制">多核调度与 NUMA 优化机制</h3>
<h4 id="多线程并行推理原理">多线程并行推理原理</h4>
<p>大语言模型的推理计算高度并行化，特别是矩阵乘法操作，可拆分为多个子块并由多个线程并行执行。借助鲲鹏920
提供的大量 CPU 核心，可显著提高推理吞吐。</p>
<p>推理框架通常通过以下两种方式实现并行调度：</p>
<ul>
<li><strong>自动线程分配</strong>：框架在启动时根据系统硬件自动确定线程数；</li>
<li><strong>显式参数配置</strong>：用户可通过环境变量或命令行设置线程并发度，精细控制资源分配。</li>
</ul>
<p>合理的多核利用有助于在保持计算正确性的同时，加快生成速度，提升整体吞吐。</p>
<h4 id="numa-架构优化原理">NUMA 架构优化原理</h4>
<p>鲲鹏920 服务器采用 NUMA（Non-Uniform Memory Access）架构，即每个 CPU
插槽（NUMA 节点）拥有独立的本地内存。CPU
访问本地内存的延迟和带宽显著优于跨节点访问。</p>
<p>若不加限制地调度线程：</p>
<ul>
<li>线程可能访问非本地内存，引发访问延迟；</li>
<li>多节点间需维护缓存一致性，增加调度与同步开销；</li>
<li>内存带宽冲突加剧，降低整体性能。</li>
</ul>
<p>因此，优化 NUMA 亲和性策略至关重要，包括：</p>
<ul>
<li>将线程优先绑定至同一 NUMA 节点（如 node0 或 node1）；</li>
<li>使用 <code>taskset</code>​ 显式限定 CPU
核心范围，确保线程集中调度；</li>
<li>借助 <code>numactl</code>​
工具设置内存分配策略，确保本地内存优先使用。</li>
</ul>
<p>通过上述手段，可有效提升缓存命中率、减少内存冲突，从而显著增强 LLM
推理在 CPU 上的性能表现。</p>
<figure>
<img src="/images/image-20250519152933-4k448rk.png" alt="image">
<figcaption aria-hidden="true">image</figcaption>
</figure>
<p>图2 NUMA架构图</p>
<h3 id="推理加速优化方法原理">推理加速优化方法原理</h3>
<p>在保持模型结构与功能不变的前提下，本实验从以下三个方向系统性优化了大型语言模型在
CPU 环境下的推理性能：</p>
<h4 id="neon-向量化加速simd">NEON 向量化加速（SIMD）</h4>
<p>SIMD（Single Instruction Multiple
Data）是一种通过单条指令并行处理多个数据的技术，ARM 架构中的 NEON
指令集即为其典型实现。在传统 CPU 中，例如执行 4 次浮点加法通常需 4
条指令，而使用 NEON，仅需一条 <code>vaddq_f32</code>​
指令即可完成相同运算，显著减少指令数并提高每周期吞吐量。</p>
<p>在大语言模型推理过程中，矩阵乘法、向量加和、归一化等操作广泛存在，这些计算高度契合
SIMD 并行处理方式。NEON 指令特别适用于如下模型模块：</p>
<ul>
<li><strong>前馈神经网络（MLP）层</strong>：大规模矩阵与向量乘法操作；</li>
<li><strong>注意力机制中的 QKV
运算</strong>：需大量并行向量点乘与归一化；</li>
<li><strong>激活函数与归一化计算</strong>：如 GELU、LayerNorm 等。</li>
</ul>
<p>通过充分利用 NEON 向量指令，模型推理在 CPU
上的执行效率得到了显著提升。</p>
<figure>
<img src="/images/image-20250519153000-wb5svxo.png" alt="image">
<figcaption aria-hidden="true">image</figcaption>
</figure>
<p>图3 SIMD原理图</p>
<h4 id="模型量化quantization">模型量化（Quantization）</h4>
<p>模型量化是一种在不改变网络结构和功能的前提下，将高精度浮点参数（如
FP32 或 FP16）转换为低位宽整数（如
INT8、INT4、INT3）的推理加速方法。其核心优势在于降低模型存储体积与计算复杂度，同时借助
SIMD 指令集在整数域内加速矩阵运算。</p>
<ul>
<li><p>量化基本流程</p>
<ol type="1">
<li><strong>线性映射（Quantization）</strong><br>
将浮点数按比例因子 <code>scale</code>​ 映射为目标整数值，必要时引入偏移量
<code>zero-point</code>​ 处理非对称分布。</li>
<li><strong>低精度计算</strong><br>
在 INT8
或更低精度下完成矩阵乘法、加法等核心操作，显著降低指令复杂度与内存占用。</li>
<li><strong>反量化（Dequantization）</strong><br>
将输出乘以 <code>scale</code>​ 并加回
<code>zero-point</code>​，还原近似浮点结果供下一层处理。</li>
</ol></li>
<li><p>常见量化方式</p>
<ul>
<li><p><strong>对称量化</strong>：仅使用
<code>scale</code>​，适用于均值接近 0 的权重。</p>
<figure>
<img src="/images/image-20250519153707-c2iy995.png" alt="image">
<figcaption aria-hidden="true">image</figcaption>
</figure></li>
<li><p><strong>非对称量化</strong>：引入
<code>zero-point</code>​，适配偏移分布的输入/权重。</p>
<figure>
<img src="/images/image-20250519153720-g9pab7e.png" alt="image">
<figcaption aria-hidden="true">image</figcaption>
</figure></li>
<li><p><strong>Per-Channel / Per-Group
量化</strong>：为不同通道或块分配独立
<code>scale</code>​，保留细粒度动态范围。</p>
<figure>
<img src="/images/image-20250519153730-phdhbm8.png" alt="image">
<figcaption aria-hidden="true">image</figcaption>
</figure></li>
<li><p><strong>K-Means
聚类量化</strong>：将权重聚类后用索引编码，结合查表操作进一步压缩，如
<code>q4_K_M</code>​、<code>q3_K_S</code>​。</p>
<figure>
<img src="/images/image-20250519153738-9w5d2wb.png" alt="image">
<figcaption aria-hidden="true">image</figcaption>
</figure></li>
</ul></li>
</ul>
<p>本实验采用 Ollama 框架，基于 GGUF 模型格式部署
<code>DeepSeek-R1-Distill-Qwen-1.5B</code>​，通过命令行参数
<code>--quantize</code>​ 实现从 FP16 到 INT3 多种位宽的推理加速。</p>
<p>测试模型版本涵盖：</p>
<ul>
<li>非量化（FP16）</li>
<li>对称整数量化（q8、q5、q4、q3）</li>
</ul>
<p>后续将在第 4.5.1.2 节中系统对比不同量化等级下的性能与精度差异。</p>
<h4 id="kv-缓存与增量生成">KV 缓存与增量生成</h4>
<p>Transformer 模型的文本生成过程为逐 token 推理，每次新生成一个 token
时都需基于全部历史上下文重新计算注意力，这在长文本场景下会显著增加计算开销</p>
<p>为解决上述问题，推理框架引入了 KV 缓存（Key-Value
Cache）机制，其基本原理如下：</p>
<ul>
<li>在生成首个 token 时，计算并缓存所有层的 K（Key）和
V（Value）向量；</li>
<li>后续每生成一个新 token，仅需计算对应的 Q（Query）并与已有 K/V
进行注意力运算；</li>
<li>避免重复回算历史 token 的注意力，显著降低每轮计算量。</li>
</ul>
<p>Ollama 基于 llama.cpp 实现了对 KV
缓存的自动支持，用户无需手动干预。在配合良好的内存布局与线程调度策略下，该机制可极大提升生成响应速度，尤其适用于多轮长文本推理任务。</p>
<h2 id="实践过程与结果">实践过程与结果</h2>
<h3 id="服务器选型与环境配置">服务器选型与环境配置</h3>
<p>本次实验选用了基于鲲鹏920处理器的鲲鹏云服务器，具体型号为
<strong>内存优化型</strong>
<strong>KM1</strong>，其具备以下配置优势：</p>
<p><strong>48核心</strong>
<strong>vCPUs</strong>：满足大语言模型推理所需的多线程高并发计算能力。</p>
<p><strong>384GiB超大内存</strong>：适配大模型运行过程中的 KV
缓存与矩阵运算内存占用。</p>
<p><strong>系统盘</strong>
<strong>128GiB</strong>：满足系统与模型部署需求。</p>
<p><strong>操作系统为</strong>
<strong>openEuler20.03LTSSP1（ARM64bit）</strong>
：兼容性与国产生态良好。</p>
<p><strong>公网访问能力</strong>：申请弹性公网 IP，支持外部连接与部署
WebUI。</p>
<table>
<thead>
<tr>
<th><strong>配置项</strong></th>
<th><strong>配置详情</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td>CPU型号</td>
<td>华为鲲鹏920（ARMv8架构）</td>
</tr>
<tr>
<td>vCPU型号</td>
<td>48核</td>
</tr>
<tr>
<td>内存</td>
<td>384GiB</td>
</tr>
<tr>
<td>系统盘</td>
<td>128GiB SSD</td>
</tr>
<tr>
<td>操作系统</td>
<td>openEuler 20.03 LTS SP1(ARM 64bit)</td>
</tr>
<tr>
<td>公网访问</td>
<td>已配置弹性公网IP，支持远程访问</td>
</tr>
</tbody>
</table>
<h3 id="ollama-安装与配置">Ollama 安装与配置</h3>
<p>Ollama 是一个轻量级本地部署框架，底层封装了如 llama.cpp
等高效推理引擎，其具备以下优势：</p>
<p>✅ <strong>跨平台支持</strong>：兼容 Linux / Windows / macOS；</p>
<p>✅ <strong>模型多样性</strong>：支持 LLaMA、DeepSeek、Phi、Gemma
等多个主流开源模型；</p>
<p>✅ <strong>硬件灵活性</strong>：支持 CPU 和 GPU
部署，适用于资源受限环境。</p>
<p>由于鲲鹏920 服务器不具备 GPU，仅提供 CPU 推理能力，因此 Ollama
的安装与优化需充分考虑 ARM 架构与多核并行性能。</p>
<p><strong>安装步骤概览</strong></p>
<table>
<thead>
<tr>
<th>操作步骤</th>
<th>说明</th>
</tr>
</thead>
<tbody>
<tr>
<td>创建安装目录</td>
<td>准备部署环境</td>
</tr>
<tr>
<td>下载并上传安装包</td>
<td>获取 ARM 架构版本的安装包</td>
</tr>
<tr>
<td>解压并进入目录</td>
<td>配置运行路径</td>
</tr>
<tr>
<td>配置环境变量</td>
<td>便于系统全局调用 Ollama</td>
</tr>
<tr>
<td>升级 GCC 与依赖</td>
<td>解决兼容性问题</td>
</tr>
<tr>
<td>配置 Systemd 服务</td>
<td>支持后台运行与自启动</td>
</tr>
</tbody>
</table>
<h4 id="创建安装目录">创建安装目录</h4>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">mkdir</span> /opt/ollama</span><br></pre></td></tr></table></figure>
<hr>
<h4 id="下载并上传安装包">下载并上传安装包</h4>
<p>从官网获取 ARM64 安装包：<a target="_blank" rel="noopener" href="https://ollama.com/download/ollama-linux-arm64.tgz">Ollama
官方下载链接</a></p>
<p>使用 <code>scp</code>​ 命令将安装包上传到服务器：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scp your_path/ollama-linux-arm64.tgz root@&lt;server_ip&gt;:/opt/ollama/</span><br></pre></td></tr></table></figure>
<hr>
<h4 id="解压并进入目录">解压并进入目录</h4>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> /opt/ollama</span><br><span class="line">tar -xzvf ollama-linux-arm64.tgz</span><br></pre></td></tr></table></figure>
<hr>
<h4 id="配置环境变量">配置环境变量</h4>
<p>将 Ollama 路径加入系统环境变量，方便直接调用命令：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vim /etc/profile</span><br></pre></td></tr></table></figure>
<p>在文件末尾添加：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">export</span> OLLAMA_HOME=/opt/ollama</span><br><span class="line"><span class="built_in">export</span> PATH=<span class="variable">$OLLAMA_HOME</span>/bin:<span class="variable">$PATH</span></span><br></pre></td></tr></table></figure>
<p>保存并退出（vim 中输入 <code>Esc</code>​ →
<code>:wq</code>​），然后使配置生效：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">source</span> /etc/profile</span><br></pre></td></tr></table></figure>
<p>验证配置是否成功：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">echo</span> <span class="variable">$OLLAMA_HOME</span></span><br><span class="line"><span class="built_in">which</span> ollama</span><br></pre></td></tr></table></figure>
<p>如返回 <code>/opt/ollama/bin/ollama</code>​，说明配置正确。</p>
<hr>
<h3 id="解决-gcc-兼容性问题如遇-glibcxx_3.4.25-not-found">解决 GCC
兼容性问题（如遇 <code>GLIBCXX_3.4.25 not found</code>​）</h3>
<h4 id="安装构建依赖">安装构建依赖：</h4>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">sudo</span> dnf install -y gcc gcc-c++ make wget tar bzip2</span><br></pre></td></tr></table></figure>
<h4 id="下载并编译-gcc-10.5">下载并编译 GCC 10.5：</h4>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> /usr/local/src</span><br><span class="line">wget http://ftp.gnu.org/gnu/gcc/gcc-10.5.0/gcc-10.5.0.tar.gz</span><br><span class="line">tar -xvzf gcc-10.5.0.tar.gz</span><br><span class="line"><span class="built_in">cd</span> gcc-10.5.0</span><br><span class="line">./contrib/download_prerequisites</span><br><span class="line"><span class="built_in">mkdir</span> build &amp;&amp; <span class="built_in">cd</span> build</span><br><span class="line">../configure --disable-multilib --enable-languages=c,c++</span><br><span class="line">make -j$(<span class="built_in">nproc</span>)         <span class="comment"># 编译时间较长，视性能而定（约 30min～2h）</span></span><br><span class="line"><span class="built_in">sudo</span> make install</span><br></pre></td></tr></table></figure>
<h4 id="替换-libstdc-版本如有需要">替换 libstdc++
版本（如有需要）：</h4>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">sudo</span> <span class="built_in">ln</span> -sf /usr/local/lib64/libstdc++.so.6 /usr/lib64/libstdc++.so.6</span><br></pre></td></tr></table></figure>
<hr>
<h3 id="注册-systemd-后台服务">注册 Systemd 后台服务</h3>
<p>为了实现 Ollama 常驻运行与开机自启，可将其注册为 Systemd 服务。</p>
<h4 id="创建服务配置文件">创建服务配置文件：</h4>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cat</span> &lt;&lt; <span class="string">'EOF'</span> | <span class="built_in">sudo</span> <span class="built_in">tee</span> /etc/systemd/system/ollama.service &gt; /dev/null</span><br><span class="line">[Unit]</span><br><span class="line">Description=Ollama Service</span><br><span class="line">After=network-online.target</span><br><span class="line"></span><br><span class="line">[Service]</span><br><span class="line">ExecStart=/opt/ollama/bin/ollama serve</span><br><span class="line">User=root</span><br><span class="line">Group=root</span><br><span class="line">Restart=always</span><br><span class="line">RestartSec=3</span><br><span class="line">Environment=<span class="string">"OLLAMA_HOST=0.0.0.0"</span></span><br><span class="line">Environment=<span class="string">"OLLAMA_KEEP_ALIVE=-1"</span></span><br><span class="line">Environment=<span class="string">"OLLAMA_MODELS=/opt/ollama/models"</span></span><br><span class="line">Environment=<span class="string">"PATH=<span class="variable">$PATH</span>"</span></span><br><span class="line"></span><br><span class="line">[Install]</span><br><span class="line">WantedBy=default.target</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure>
<h4 id="启用并启动服务">启用并启动服务：</h4>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">sudo</span> systemctl daemon-reload         <span class="comment"># 重新加载服务配置</span></span><br><span class="line"><span class="built_in">sudo</span> systemctl <span class="built_in">enable</span> ollama         <span class="comment"># 设置开机启动</span></span><br><span class="line"><span class="built_in">sudo</span> systemctl start ollama          <span class="comment"># 启动服务</span></span><br></pre></td></tr></table></figure>
<h4 id="查看服务状态">查看服务状态：</h4>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">sudo</span> systemctl status ollama</span><br></pre></td></tr></table></figure>
<p>如输出中包含 <code>Active: active (running)</code>​，说明 Ollama
已成功启动并在后台运行。</p>
<h3 id="ollama-运行多模型部署与对比实验">Ollama
运行：多模型部署与对比实验</h3>
<p>Ollama
支持运行多种主流开源大模型，具备良好的模型管理与多版本运行能力。本实验选取
<code>DeepSeek-R1</code>​ 系列中两个不同规模的模型（1.5B 与
7B）进行部署测试，并通过详细运行日志对比其性能差异。</p>
<hr>
<h4 id="模型运行测试deepseek-r1-1.5b">模型运行测试：DeepSeek-R1-1.5B</h4>
<p>执行命令：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ollama run deepseek-r1:1.5b --verbose</span><br></pre></td></tr></table></figure>
<p>该命令将在终端启动与模型的对话界面，并通过 <code>--verbose</code>​
参数输出关键性能指标。</p>
<p>示例输出如下：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">total duration:         699.94 ms</span><br><span class="line">load duration:          23.30 ms</span><br><span class="line">prompt eval count:      5 tokens</span><br><span class="line">prompt eval duration:   57.04 ms</span><br><span class="line">prompt eval rate:       87.66 tokens/s</span><br><span class="line">eval count:             13 tokens</span><br><span class="line">eval duration:          618.86 ms</span><br><span class="line">eval rate:              21.01 tokens/s</span><br></pre></td></tr></table></figure>
<hr>
<h4 id="模型运行测试deepseek-r1-7b">模型运行测试：DeepSeek-R1-7B</h4>
<p>执行命令：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ollama run deepseek-r1:7b --verbose</span><br></pre></td></tr></table></figure>
<p>输出示例如下：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">total duration:         71.33 s</span><br><span class="line">load duration:          23.44 ms</span><br><span class="line">prompt eval count:      54 tokens</span><br><span class="line">prompt eval duration:   372.06 ms</span><br><span class="line">prompt eval rate:       145.14 tokens/s</span><br><span class="line">eval count:             587 tokens</span><br><span class="line">eval duration:          70.93 s</span><br><span class="line">eval rate:              8.28 tokens/s</span><br></pre></td></tr></table></figure>
<hr>
<h4 id="模型性能对比分析">模型性能对比分析</h4>
<p>‍</p>
<table>
<colgroup>
<col style="width: 3%">
<col style="width: 21%">
<col style="width: 23%">
<col style="width: 31%">
<col style="width: 19%">
</colgroup>
<thead>
<tr>
<th><strong>模型规模</strong></th>
<th><strong>总耗时 (total duration)</strong></th>
<th><strong>推理 tokens 数</strong></th>
<th><strong>推理速度 (eval rate)</strong></th>
<th><strong>加载耗时</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>1.5B</strong></td>
<td>699.94 ms</td>
<td>13 tokens</td>
<td>21.01 tokens/s</td>
<td>23.30 ms</td>
</tr>
<tr>
<td><strong>7B</strong></td>
<td>71.33 s</td>
<td>587 tokens</td>
<td>8.28 tokens/s</td>
<td>23.44 ms</td>
</tr>
</tbody>
</table>
<blockquote>
<p>表格 3：不同规模模型推理性能对比</p>
</blockquote>
<ul>
<li>✅ <strong>1.5B
小模型</strong>：加载快、推理时间短，适合轻量级问答或边缘设备部署；</li>
<li>⏳ <strong>7B
大模型</strong>：虽然支持更复杂的语义理解，但推理耗时明显增加，单位
token 生成速度下降，更适合对质量要求较高的任务场景。</li>
</ul>
<p>从本次实验对比可以看出，模型规模与运行性能呈明显反比关系。部署时需根据具体业务需求权衡选择：</p>
<ul>
<li><strong>追求响应速度与资源节省</strong>：建议选用 1.5B
等小模型；</li>
<li><strong>追求语义能力与上下文理解深度</strong>：可考虑部署 7B
或更大规模模型，但需配合 NUMA 优化与量化技术以提高运行效率。</li>
</ul>
<figure>
<img src="/images/image-20250519154340-vjon8ro.png" alt="image">
<figcaption aria-hidden="true">image</figcaption>
</figure>
<h3 id="ollama-性能调优">Ollama 性能调优</h3>
<h4 id="线程数设置">线程数设置</h4>
<p>Ollama 支持通过设置 <code>num_thread</code>​ 参数控制底层
<code>llama.cpp</code>​
的线程并行度。合理配置线程数对于提升模型推理效率具有重要意义。</p>
<p>设置命令示例如下：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; /set parameter num_thread 48</span><br><span class="line">ollama run deepseek-r1 --verbose</span><br></pre></td></tr></table></figure>
<h4 id="实验数据对比">实验数据对比</h4>
<table>
<thead>
<tr>
<th>参数配置</th>
<th>​<code>num_thread = 48</code>​</th>
<th>​<code>num_thread = 24</code>​</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>总耗时</strong></td>
<td>6.85 s</td>
<td>8.25 s</td>
</tr>
<tr>
<td><strong>加载耗时</strong></td>
<td>1.66 s</td>
<td>1.66 s</td>
</tr>
<tr>
<td><strong>Prompt 评估数</strong></td>
<td>5 tokens</td>
<td>49 tokens</td>
</tr>
<tr>
<td><strong>Prompt 速度</strong></td>
<td>24.96 tokens/s</td>
<td>19.50 tokens/s</td>
</tr>
<tr>
<td><strong>推理 token 数</strong></td>
<td>40 tokens</td>
<td>40 tokens</td>
</tr>
<tr>
<td><strong>推理速度（Eval）</strong></td>
<td>8.01 tokens/s</td>
<td><strong>9.82 tokens/s</strong></td>
</tr>
</tbody>
</table>
<p><strong>通过分析可得：</strong></p>
<ul>
<li>虽然线程数 48 总时延略优，但生成速率（eval rate）在 24
线程时反而更高；</li>
<li>可能的原因是，48 线程可能导致线程分配至两个NUMA
节点，引入跨节点访问延迟；</li>
<li>24 线程可能恰好约束在单个NUMA 节点内，缓存命中高，效率反而优。</li>
</ul>
<p><strong>所以</strong>，并行线程 ≠ 越多越好，在 NUMA
架构下尤其需权衡内存访问位置与线程负载。</p>
<figure>
<img src="/images/image-20250519154538-u06qkom.png" alt="image">
<figcaption aria-hidden="true">image</figcaption>
</figure>
<h4 id="numa-架构优化线程绑定与亲和性调度">NUMA
架构优化：线程绑定与亲和性调度</h4>
<p>在 NUMA（非一致内存访问）系统中，不同 CPU
核心访问自身“本地”内存比访问远程节点更快。因此，将线程绑定至同一 NUMA
节点是提高性能的关键策略之一。</p>
<h5 id="查看-numa-拓扑结构">查看 NUMA 拓扑结构</h5>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">lscpu | grep <span class="string">"NUMA node"</span></span><br></pre></td></tr></table></figure>
<p>示例输出：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">NUMA node(s):        2</span><br><span class="line">NUMA node0 CPU(s):   0-23</span><br><span class="line">NUMA node1 CPU(s):   24-47</span><br></pre></td></tr></table></figure>
<h5 id="绑定线程到特定-numa-节点">绑定线程到特定 NUMA 节点</h5>
<p>使用 <code>taskset</code>​ 命令可将程序固定运行在指定核心集上：</p>
<ul>
<li><strong>绑定 NUMA Node0（0–23）核心：</strong></li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">taskset -c 0-23 ollama run deepseek-r1:7b --verbose</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">eval rate: 8.17 tokens/s</span><br><span class="line">total duration: 5.20 s</span><br></pre></td></tr></table></figure>
<ul>
<li><strong>绑定 NUMA Node1（24–47）核心：</strong></li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">taskset -c 24-47 ollama run deepseek-r1:7b --verbose</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">eval rate: 8.32 tokens/s</span><br><span class="line">total duration: 4.94 s</span><br></pre></td></tr></table></figure>
<ul>
<li><strong>绑定全核（跨 NUMA 节点，0–47）：</strong></li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">taskset -c 0-47 ollama run deepseek-r1:7b --verbose</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">eval rate: 8.29 tokens/s</span><br><span class="line">total duration: 7.82 s</span><br></pre></td></tr></table></figure>
<h5 id="多策略对比分析表">多策略对比分析表</h5>
<table>
<colgroup>
<col style="width: 36%">
<col style="width: 8%">
<col style="width: 8%">
<col style="width: 30%">
<col style="width: 16%">
</colgroup>
<thead>
<tr>
<th><strong>绑定策略</strong></th>
<th><strong>核心数</strong></th>
<th><strong>是否跨 NUMA</strong></th>
<th><strong>推理速度（Eval Rate）</strong></th>
<th><strong>总耗时</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td>Node0（0–23）</td>
<td>24</td>
<td>否</td>
<td>8.17 tokens/s</td>
<td>5.20 s</td>
</tr>
<tr>
<td>Node1（24–47）</td>
<td>24</td>
<td>否</td>
<td><strong>8.32 tokens/s</strong></td>
<td><strong>4.94 s</strong></td>
</tr>
<tr>
<td>跨 NUMA（0–47）</td>
<td>48</td>
<td>是</td>
<td>8.29 tokens/s</td>
<td>7.82 s</td>
</tr>
</tbody>
</table>
<p><strong>通过分析可得：</strong></p>
<ul>
<li>Node1 单节点表现最佳，推理速度和响应时间均优于其他策略；</li>
<li>跨 NUMA 尽管线程数翻倍，但未带来线性加速，反而在 total duration
上性能下降；</li>
<li>验证了 NUMA 架构中，合理线程绑定远比简单堆砌核心数更有效。</li>
</ul>
<p><strong>所以，</strong> 在部署大语言模型至 CPU
侧时，需充分考虑硬件结构，线程与NUMA
核心绑定是一种性价比极高的性能提升手段。</p>
<figure>
<img src="/images/image-20250519154943-njrrexc.png" alt="image">
<figcaption aria-hidden="true">image</figcaption>
</figure>
<h4 id="int量化与-simd-优化分析">INT量化与 SIMD 优化分析</h4>
<h5 id="int-量化原理与效果">INT 量化原理与效果</h5>
<p><strong>量化（Quantization）</strong>
是将模型中高精度浮点型参数压缩为低精度整数（如
INT8、INT4）的技术，以降低模型体积、加快推理速度。</p>
<table>
<colgroup>
<col style="width: 15%">
<col style="width: 21%">
<col style="width: 23%">
<col style="width: 23%">
<col style="width: 15%">
</colgroup>
<thead>
<tr>
<th><strong>精度类型</strong></th>
<th><strong>单参数位宽</strong></th>
<th><strong>7B 模型体积</strong></th>
<th><strong>推理内存需求</strong></th>
<th><strong>典型推理吞吐率 (tok/s)</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td>FP32</td>
<td>32-bit</td>
<td>≈28 GB</td>
<td>≈30 GB</td>
<td>2–3</td>
</tr>
<tr>
<td>FP16</td>
<td>16-bit</td>
<td>≈14 GB</td>
<td>≈15 GB</td>
<td>4–5</td>
</tr>
<tr>
<td>INT8</td>
<td>8-bit</td>
<td>≈7 GB</td>
<td>≈8 GB</td>
<td>7–8</td>
</tr>
<tr>
<td><strong>INT4</strong></td>
<td><strong>4-bit</strong></td>
<td><strong>4.36 GB (实测)</strong></td>
<td><strong>≈5 GB</strong></td>
<td><strong>≈9</strong></td>
</tr>
</tbody>
</table>
<p>本实验中，部署的 DeepSeek-R1-7B 模型采用 INT4 量化，文件大小仅为
<strong>4.36 GB</strong>（约 4,683,073,184
字节），相较于未量化模型（通常 13~20
GB），<strong>显著降低了内存压力与加载延迟</strong>。</p>
<h5 id="各量化版本对比分析">各量化版本对比分析</h5>
<ul>
<li><p>获取 Hugging Face 模型并上传服务器</p>
<p>从 Hugging Face 获取 DeepSeek-R1 模型权重：<br>
👉 <a target="_blank" rel="noopener" href="https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B">https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B</a></p>
<p>使用 Git LFS 下载后，通过 <code>scp</code>​ 上传至鲲鹏 920
服务器：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scp -r ./DeepSeek-R1-Distill-Qwen-1.5B root@117.78.10.187:/opt/ollama/</span><br></pre></td></tr></table></figure></li>
<li><p>安装 Python 3.9</p>
<p>鲲鹏服务器默认 Python 版本较低，不支持海象运算符
<code>:=</code>​，需升级至 3.9 以上。</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">sudo</span> yum install -y gcc make zlib-devel openssl-devel bzip2-devel libffi-devel --nogpgcheck</span><br><span class="line">wget https://www.python.org/ftp/python/3.9.19/Python-3.9.19.tgz</span><br><span class="line">tar xzf Python-3.9.19.tgz &amp;&amp; <span class="built_in">cd</span> Python-3.9.19</span><br><span class="line">./configure --prefix=<span class="variable">$HOME</span>/python39 --enable-optimizations</span><br><span class="line">make -j$(<span class="built_in">nproc</span>) &amp;&amp; make install</span><br><span class="line"></span><br><span class="line"><span class="built_in">echo</span> <span class="string">'export PATH="$HOME/python39/bin:$PATH"'</span> &gt;&gt; ~/.bashrc</span><br><span class="line"><span class="built_in">source</span> ~/.bashrc</span><br><span class="line">python3 --version  <span class="comment"># 应为 Python 3.9.19</span></span><br></pre></td></tr></table></figure></li>
<li><p>配置运行环境</p>
<p>升级 <code>libstdc++</code>​（需版本 ≥ 3.4.25）：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">export</span> LD_LIBRARY_PATH=/usr/local/lib64:<span class="variable">$LD_LIBRARY_PATH</span></span><br></pre></td></tr></table></figure></li>
<li><p>安装 ARM 架构 Python 依赖</p>
<ul>
<li>安装 Numpy（源码编译）：</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">git <span class="built_in">clone</span> https://github.com/numpy/numpy.git</span><br><span class="line"><span class="built_in">cd</span> numpy &amp;&amp; git checkout v1.26.4</span><br><span class="line">pip3 install .</span><br></pre></td></tr></table></figure>
<ul>
<li>安装 PyTorch（适配 ARM 架构）：</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip3 install torch-2.1.0-cp39-cp39-linux_aarch64.whl</span><br></pre></td></tr></table></figure>
<ul>
<li>安装 SentencePiece：</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip3 install sentencepiece-0.2.0-cp39-cp39-manylinux2014_aarch64.whl</span><br></pre></td></tr></table></figure>
<ul>
<li>安装 PyYAML：</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip3 install PyYAML-6.0.2-cp39-cp39-manylinux2014_aarch64.whl</span><br></pre></td></tr></table></figure></li>
<li><p>编译并升级 GCC</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">cd /opt</span><br><span class="line">wget https://ftp.gnu.org/gnu/gcc/gcc-<span class="number">9.5</span><span class="number">.0</span>/gcc-<span class="number">9.5</span><span class="number">.0</span>.tar.gz</span><br><span class="line">tar -xzf gcc-<span class="number">9.5</span><span class="number">.0</span>.tar.gz &amp;&amp; cd gcc-<span class="number">9.5</span><span class="number">.0</span></span><br><span class="line">./contrib/download_prerequisites</span><br><span class="line">mkdir build &amp;&amp; cd build</span><br><span class="line">../configure --disable-multilib --enable-languages=c,c++</span><br><span class="line">make -j$(nproc) &amp;&amp; make install</span><br><span class="line">sudo ln -sf /usr/local/lib64/libstdc++.so<span class="number">.6</span> /usr/lib64/libstdc++.so<span class="number">.6</span></span><br></pre></td></tr></table></figure></li>
<li><p>模型转换为 GGUF 格式</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">git clone https://github.com/ggml-org/llama.cpp &amp;&amp; cd llama.cpp &amp;&amp; make</span><br><span class="line">python3 convert-llama-to-gguf.py \</span><br><span class="line">  --model_path /opt/DeepSeek-R1-7B/model.safetensors \</span><br><span class="line">  --quantize <span class="number">8</span> \</span><br><span class="line">  --output /opt/ollama/ds_int8.gguf</span><br></pre></td></tr></table></figure></li>
</ul>
<h5 id="量化模型对比">量化模型对比</h5>
<ul>
<li><p><strong>非量化版本（FP16 模型）</strong></p>
<p><strong>模型压缩：</strong> 使用未量化的 FP16 格式 GGUF
模型，无进一步量化处理。</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">echo</span> <span class="string">"FROM /opt/ollama/ds.gguf"</span> &gt; Modelfile</span><br><span class="line">ollama create deepseek-r1:7b -f Modelfile</span><br><span class="line">ollama run deepseek-r1:7b --verbose</span><br></pre></td></tr></table></figure>
<p><strong>推理效果：</strong></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">total duration:        1m21.131s</span><br><span class="line">load duration:         24.111 ms</span><br><span class="line">prompt eval count:     5 tokens</span><br><span class="line">prompt eval duration:  224.180 ms</span><br><span class="line">prompt eval rate:      22.30 tokens/s</span><br><span class="line">eval count:            394 tokens</span><br><span class="line">eval duration:         1m20.882s</span><br><span class="line">eval rate:             4.87 tokens/s</span><br></pre></td></tr></table></figure>
<p><strong>分析：</strong></p>
<ul>
<li>模型权重以 IEEE 754 半精度浮点（16-bit）格式存储：1 位符号 + 5
位指数 + 10
位尾数，无压缩或量化处理，<strong>保持完整的训练精度</strong>；</li>
<li><strong>推理速度为 4.87
tokens/s</strong>，为所有方案中最慢。主要瓶颈来自<strong>高内存带宽占用与计算资源需求</strong>；</li>
<li>适合部署在<strong>资源充裕、高精度需求的场景</strong>。</li>
</ul></li>
<li><p><strong>q8_0 量化版本（8-bit 对称量化）</strong></p>
<p><strong>模型压缩：</strong></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">echo</span> <span class="string">"FROM /opt/ollama/ds.gguf"</span> &gt; Modelfile_q8_0</span><br><span class="line">ollama create mymodel-q8_0 -f Modelfile_q8_0 --quantize q8_0</span><br><span class="line">ollama run mymodel-q8_0 --verbose</span><br></pre></td></tr></table></figure>
<p><strong>推理效果：</strong></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">total duration:        1m1.550s</span><br><span class="line">load duration:         24.051 ms</span><br><span class="line">prompt eval count:     7 tokens</span><br><span class="line">prompt eval duration:  280.707 ms</span><br><span class="line">prompt eval rate:      24.94 tokens/s</span><br><span class="line">eval count:            373 tokens</span><br><span class="line">eval duration:         1m1.245s</span><br><span class="line">eval rate:             6.09 tokens/s</span><br></pre></td></tr></table></figure>
<p><strong>分析：</strong></p>
<ul>
<li>采用 <strong>8-bit 对称量化</strong>，所有权重线性映射至整数区间
[-128, 127]；</li>
<li>量化公式：<code>w_int8 = round((w - zero_point) / scale)</code>​，反量化为
<code>w_fp32 = w_int8 * scale + zero_point</code>​；</li>
<li>推理速度提升至 <strong>6.09 tokens/s</strong>，明显优于
FP16，且仍保持较高精度；</li>
<li>适用于对速度有提升要求、但精度损失不能太大的场景。</li>
</ul></li>
<li><p><strong>q5_1 量化版本（5-bit 非对称量化）</strong></p>
<p><strong>模型压缩：</strong></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">echo</span> <span class="string">"FROM /opt/ollama/ds.gguf"</span> &gt; Modelfile_q5_1</span><br><span class="line">ollama create mymodel-q5_1 -f Modelfile_q5_1 --quantize q5_1</span><br><span class="line">ollama run mymodel-q5_1 --verbose</span><br></pre></td></tr></table></figure>
<p><strong>推理效果：</strong></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">total duration:        7m18.801s</span><br><span class="line">load duration:         24.206 ms</span><br><span class="line">prompt eval count:     6 tokens</span><br><span class="line">prompt eval duration:  214.139 ms</span><br><span class="line">prompt eval rate:      28.02 tokens/s</span><br><span class="line">eval count:            2630 tokens</span><br><span class="line">eval duration:         7m18.562s</span><br><span class="line">eval rate:             6.00 tokens/s</span><br></pre></td></tr></table></figure>
<p><strong>分析：</strong></p>
<ul>
<li>支持 per-group scale + zero-point
非对称量化，按通道或块划分精细压缩；</li>
<li>虽精度高于对称量化，但推理复杂度大、内存访问更频繁；</li>
<li><strong>速度为 6.00
tokens/s</strong>，<strong>总耗时最长</strong>，适合对精度要求高、可接受延迟的场景。</li>
</ul></li>
<li><p><strong>q5_0 量化版本（5-bit 对称量化）</strong></p>
<p><strong>模型压缩：</strong></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">echo</span> <span class="string">"FROM /opt/ollama/ds.gguf"</span> &gt; Modelfile_q5_0</span><br><span class="line">ollama create mymodel-q5_0 -f Modelfile_q5_0 --quantize q5_0</span><br><span class="line">ollama run mymodel-q5_0 --verbose</span><br></pre></td></tr></table></figure>
<p><strong>推理效果：</strong></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">total duration:        1m2.392s</span><br><span class="line">load duration:         24.106 ms</span><br><span class="line">prompt eval count:     6 tokens</span><br><span class="line">prompt eval duration:  106.868 ms</span><br><span class="line">prompt eval rate:      56.14 tokens/s</span><br><span class="line">eval count:            384 tokens</span><br><span class="line">eval duration:         1m2.260s</span><br><span class="line">eval rate:             6.17 tokens/s</span><br></pre></td></tr></table></figure>
<p><strong>分析：</strong></p>
<ul>
<li>与 q5_1 类似，但 <strong>不使用
zero-point</strong>，即为标准对称量化；</li>
<li>结构简洁，适合加速指令实现；</li>
<li><strong>推理速度为 6.17 tokens/s</strong>，略优于
q5_1，推荐用于资源受限但对精度仍有要求的场景。</li>
</ul></li>
<li><p><strong>q4_1 量化版本（4-bit 非对称）</strong></p>
<p><strong>模型压缩：</strong></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">echo</span> <span class="string">"FROM /opt/ollama/ds.gguf"</span> &gt; Modelfile_q4_1</span><br><span class="line">ollama create mymodel-q4_1 -f Modelfile_q4_1 --quantize q4_1</span><br><span class="line">ollama run mymodel-q4_1 --verbose</span><br></pre></td></tr></table></figure>
<p><strong>推理效果：</strong></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">total duration:        1m56.659s</span><br><span class="line">load duration:         24.124 ms</span><br><span class="line">prompt eval count:     6 tokens</span><br><span class="line">prompt eval duration:  225.531 ms</span><br><span class="line">prompt eval rate:      26.60 tokens/s</span><br><span class="line">eval count:            785 tokens</span><br><span class="line">eval duration:         1m56.408s</span><br><span class="line">eval rate:             6.74 tokens/s</span><br></pre></td></tr></table></figure>
<p><strong>分析：</strong></p>
<ul>
<li>使用 4-bit 非对称量化，支持每组 scale + zero-point；</li>
<li>精度较高，在 4-bit 方案中表现平衡；</li>
<li><strong>速度为 6.74 tokens/s</strong>，优于所有 5-bit
版本，是低位宽中较为实用的选择。</li>
</ul></li>
<li><p><strong>q4_K_M 量化版本（4-bit K-means
聚类，中等分组）</strong></p>
<p><strong>模型压缩：</strong></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">echo</span> <span class="string">"FROM /opt/ollama/ds.gguf"</span> &gt; Modelfile_q4M</span><br><span class="line">ollama create mymodel-q4m -f Modelfile_q4M --quantize q4_K_M</span><br><span class="line">ollama run mymodel-q4m --verbose</span><br></pre></td></tr></table></figure>
<p><strong>推理效果：</strong></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">total duration:        1m59.328s</span><br><span class="line">load duration:         23.119 ms</span><br><span class="line">prompt eval count:     6 tokens</span><br><span class="line">prompt eval duration:  123.938 ms</span><br><span class="line">prompt eval rate:      48.41 tokens/s</span><br><span class="line">eval count:            817 tokens</span><br><span class="line">eval duration:         1m59.172s</span><br><span class="line">eval rate:             6.86 tokens/s</span><br></pre></td></tr></table></figure>
<p><strong>分析：</strong></p>
<ul>
<li>基于 <strong>K-means 非线性量化</strong>，每 64
个参数构成中等规模分组；</li>
<li>聚类更贴合分布，精度优于线性方案；</li>
<li><strong>推理速度为 6.86
tokens/s</strong>，在精度、速度、体积之间达到较好平衡。</li>
</ul></li>
<li><p><strong>q4_0 量化版本（4-bit 线性对称，最快）</strong></p>
<p><strong>模型压缩：</strong></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">echo</span> <span class="string">"FROM /opt/ollama/ds.gguf"</span> &gt; Modelfile_q4_0</span><br><span class="line">ollama create mymodel-q4_0 -f Modelfile_q4_0 --quantize q4_0</span><br><span class="line">ollama run mymodel-q4_0 --verbose</span><br></pre></td></tr></table></figure>
<p><strong>推理效果：</strong></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">total duration:        46.382s</span><br><span class="line">load duration:         23.566 ms</span><br><span class="line">prompt eval count:     37 tokens</span><br><span class="line">prompt eval duration:  118.444 ms</span><br><span class="line">prompt eval rate:      312.38 tokens/s</span><br><span class="line">eval count:            376 tokens</span><br><span class="line">eval duration:         46.237s</span><br><span class="line">eval rate:             8.13 tokens/s</span><br></pre></td></tr></table></figure>
<p><strong>分析：</strong></p>
<ul>
<li>最基础的 4-bit 对称量化，无 zero-point，分组共享 scale；</li>
<li>实现简单、效率极高；</li>
<li><strong>推理速度最高，达 8.13
tokens/s</strong>，但精度损失相对更大；</li>
<li>推荐用于速度优先、可接受适度精度退化的场景。</li>
</ul></li>
<li><p><strong>q3_K_L 量化版本（3-bit K-means，大分组）</strong></p>
<p><strong>模型压缩：</strong></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">echo</span> <span class="string">"FROM /opt/ollama/ds.gguf"</span> &gt; Modelfile_q3_KL</span><br><span class="line">ollama create mymodel-q3_KL -f Modelfile_q3_KL --quantize q3_K_L</span><br><span class="line">ollama run mymodel-q3_KL --verbose</span><br></pre></td></tr></table></figure>
<p><strong>推理效果：</strong></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">total duration:        1m28.772s</span><br><span class="line">load duration:         23.976 ms</span><br><span class="line">prompt eval count:     12 tokens</span><br><span class="line">prompt eval duration:  145.542 ms</span><br><span class="line">prompt eval rate:      82.45 tokens/s</span><br><span class="line">eval count:            621 tokens</span><br><span class="line">eval duration:         1m28.601s</span><br><span class="line">eval rate:             7.01 tokens/s</span><br></pre></td></tr></table></figure>
<p><strong>分析：</strong></p>
<ul>
<li>3-bit K-means 聚类，分组较大（128～256 参数）；</li>
<li>精度适中，推理效率在 3-bit 中表现突出；</li>
<li><strong>速度 7.01 tokens/s</strong>，优于多个 4/5-bit
版本，是小模型部署的平衡之选。</li>
</ul></li>
<li><p><strong>q3_K_M 量化版本（3-bit K-means，中等分组）</strong></p>
<p><strong>模型压缩：</strong></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">echo</span> <span class="string">"FROM /opt/ollama/ds.gguf"</span> &gt; Modelfile_q3_KM</span><br><span class="line">ollama create mymodel-q3_KM -f Modelfile_q3_KM --quantize q3_K_M</span><br><span class="line">ollama run mymodel-q3_KM --verbose</span><br></pre></td></tr></table></figure>
<p><strong>推理效果：</strong></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">total duration:        1m30.117s</span><br><span class="line">load duration:         24.628 ms</span><br><span class="line">prompt eval count:     11 tokens</span><br><span class="line">prompt eval duration:  360.484 ms</span><br><span class="line">prompt eval rate:      30.51 tokens/s</span><br><span class="line">eval count:            572 tokens</span><br><span class="line">eval duration:         1m29.731s</span><br><span class="line">eval rate:             6.37 tokens/s</span><br></pre></td></tr></table></figure>
<p><strong>分析：</strong></p>
<ul>
<li>分组较小，聚类更精细，提升精度；</li>
<li>计算量略大，<strong>推理速度 6.37
tokens/s</strong>，略低于大分组版本；</li>
<li>适合在精度优先的小模型部署中使用。</li>
</ul></li>
<li><p><strong>q3_K_S 量化版本（3-bit K-means，小分组）</strong></p>
<p><strong>模型压缩：</strong></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">echo</span> <span class="string">"FROM /opt/ollama/ds.gguf"</span> &gt; Modelfile_q3_KS</span><br><span class="line">ollama create mymodel-q3_KS -f Modelfile_q3_KS --quantize q3_K_S</span><br><span class="line">ollama run mymodel-q3_KS --verbose</span><br></pre></td></tr></table></figure>
<p><strong>推理效果：</strong></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">total duration:        16m32.237s</span><br><span class="line">load duration:         23.358 ms</span><br><span class="line">prompt eval count:     2 tokens</span><br><span class="line">prompt eval duration:  92.787 ms</span><br><span class="line">prompt eval rate:      21.55 tokens/s</span><br><span class="line">eval count:            5885 tokens</span><br><span class="line">eval duration:         16m32.120s</span><br><span class="line">eval rate:             5.93 tokens/s</span><br></pre></td></tr></table></figure>
<p><strong>分析：</strong></p>
<ul>
<li>极小分组、精细聚类，模型压缩极致；</li>
<li>反量化成本高，访存频繁；</li>
<li><strong>推理速度最慢，耗时超 16
分钟</strong>，适合超低内存环境但不适用于对响应时延有要求的任务。</li>
</ul></li>
<li><p>不同量化版本在推理精度和性能上各有权衡。下面的对比表汇总了上述各版本的配置及性能指标</p>
<table>
<colgroup>
<col style="width: 3%">
<col style="width: 14%">
<col style="width: 12%">
<col style="width: 5%">
<col style="width: 16%">
<col style="width: 15%">
<col style="width: 22%">
<col style="width: 1%">
<col style="width: 8%">
</colgroup>
<thead>
<tr>
<th><strong>排名</strong></th>
<th><strong>版本</strong></th>
<th><strong>精度等级</strong></th>
<th><strong>输出Tokens数</strong></th>
<th><strong>推理总耗时</strong></th>
<th><strong>方法类型</strong></th>
<th><strong>核心机制</strong></th>
<th><strong>推理速度(tokens/s)</strong></th>
<th><strong>速度等级</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>FP16</td>
<td>⭐⭐⭐⭐⭐⭐</td>
<td>587</td>
<td>≈120.5 s</td>
<td>无量化</td>
<td>原始浮点存储</td>
<td><strong>4.87</strong></td>
<td>最慢</td>
</tr>
<tr>
<td>2</td>
<td>q8_0</td>
<td>⭐⭐⭐⭐⭐</td>
<td>373</td>
<td>61.55 s</td>
<td>8-bit对称量化</td>
<td>线性整数量化</td>
<td><strong>6.09</strong></td>
<td>中等偏慢</td>
</tr>
<tr>
<td>3</td>
<td>q5_1</td>
<td>⭐⭐⭐⭐☆</td>
<td>384</td>
<td>64.0 s</td>
<td>5-bit带scale</td>
<td>per-group 缩放+偏移</td>
<td><strong>6.00</strong></td>
<td>中等偏慢</td>
</tr>
<tr>
<td>4</td>
<td>q5_0</td>
<td>⭐⭐⭐⭐</td>
<td>384</td>
<td>62.26 s</td>
<td>5-bit对称量化</td>
<td>固定scale，无zero point</td>
<td><strong>6.17</strong></td>
<td>中等</td>
</tr>
<tr>
<td>5</td>
<td>q4_1</td>
<td>⭐⭐⭐☆</td>
<td>785</td>
<td>116.66 s</td>
<td>4-bit+scale</td>
<td>精细缩放</td>
<td><strong>6.74</strong></td>
<td>中偏快</td>
</tr>
<tr>
<td>6</td>
<td>Q4_K_M</td>
<td>⭐⭐⭐</td>
<td>817</td>
<td>119.17 s</td>
<td>4-bit非线性量化</td>
<td>K-means 聚类+中分组</td>
<td><strong>6.86</strong></td>
<td>中偏快</td>
</tr>
<tr>
<td>7</td>
<td>q4_0</td>
<td>⭐⭐☆</td>
<td>376</td>
<td>46.24 s</td>
<td>4-bit线性</td>
<td>简单分组量化</td>
<td><strong>8.13</strong></td>
<td>最快</td>
</tr>
<tr>
<td>8</td>
<td>q3_K_L</td>
<td>⭐⭐</td>
<td>785</td>
<td>112.0 s</td>
<td>3-bit聚类</td>
<td>K-means+大分组</td>
<td><strong>7.01</strong></td>
<td>中快</td>
</tr>
<tr>
<td>9</td>
<td>q3_K_M</td>
<td>⭐☆</td>
<td>572</td>
<td>89.73 s</td>
<td>3-bit聚类</td>
<td>K-means+中分组</td>
<td><strong>6.37</strong></td>
<td>中等</td>
</tr>
<tr>
<td>10</td>
<td>q3_KS</td>
<td>⭐</td>
<td>5885</td>
<td>992.12 s(16m32s)</td>
<td>3-bit极限压缩</td>
<td>K-means+per-group scale</td>
<td><strong>5.93</strong></td>
<td>非常慢</td>
</tr>
</tbody>
</table></li>
</ul>
<h5 id="总结分析">总结分析</h5>
<p>在本实验的模型对比中：</p>
<ul>
<li><strong>FP16</strong>
为<strong>未量化的原始模型</strong>，以半精度浮点形式保存权重，具备最高精度，但资源开销也最大；</li>
<li>带有 **“_0” 后缀<strong>（如
<code>q8_0</code>​、<code>q5_0</code>​、<code>q4_0</code>​）的模型采用</strong>对称线性量化**，计算简洁，硬件友好；</li>
<li>带有 **“_1” 后缀<strong>（如
<code>q5_1</code>​、<code>q4_1</code>​）的模型使用</strong>逐组缩放的线性量化**，在提高表示精度的同时增加计算复杂度；</li>
<li>​<strong>​<code>q4_K</code>​</strong>​
<strong>、</strong>​<strong>​<code>q3_K_</code>​</strong> ​
<strong>系列</strong>模型采用 <strong>K-means
聚类非线性量化</strong>，通过查表方式实现极限压缩，其中
<strong>L/M/S</strong> 分别表示<strong>大/中/小分组粒度</strong>。</li>
</ul>
<p>从实验数据来看：</p>
<ul>
<li>​<strong>​<code>q4_0</code>​</strong>​ 模型以 <strong>8.13
tokens/s</strong> 的推理速度性能最佳；</li>
<li>​<strong>​<code>q3_K_L</code>​</strong>​ 模型以 <strong>7.01
tokens/s</strong> 紧随其后，表现出色；</li>
<li>这两者在保持基础语义质量的前提下，实现了<strong>显著高于其他方案的
CPU 推理吞吐率</strong>；</li>
<li>相比之下，<strong>FP16
原始模型</strong>由于未压缩、计算复杂度高，推理速度最慢；而极端压缩的
<strong>​<code>q3_K_S</code>​</strong>​
模型尽管体积最小，但反而因反量化负担重导致整体推理性能下降。</li>
</ul>
<figure>
<img src="/images/image-20250519160211-zzwrsad.png" alt="image">
<figcaption aria-hidden="true">image</figcaption>
</figure>
<p><strong>推理瓶颈分析：为何</strong> <strong>3-bit</strong>
<strong>模型反而变慢？</strong></p>
<p>尽管 3-bit 量化模型（如
<code>q3_K_M</code>​、<code>q3_K_L</code>​、<code>q3_K_S</code>​）在理论上具备最小的模型体积和最高的压缩比，原本应在资源受限的环境中展现出卓越的推理性能，但实验结果却显示其推理速度不仅未提升，反而普遍低于
4-bit 和 5-bit 方案。这一性能“反转”现象的背后，主要源于 3-bit
模型推理路径中多重复杂因素的叠加。</p>
<p>首先，3-bit 模型普遍采用 <strong>K-means
聚类量化</strong>与<strong>非线性编码机制</strong>。这类机制在压缩阶段可以极大缩减模型体积，但在推理时却无法像对称线性整数量化那样直接进行矩阵乘法运算。相反，每次推理时都必须通过查表方式从编码索引中恢复出对应的聚类中心值，然后再经过一系列乘加与还原计算。这种复杂的反量化链条显著拉长了计算路径，导致整体执行效率下降，尤其在缺乏专用指令支持的
CPU 环境中更为明显。</p>
<p>其次，由于 3-bit 编码最多只能表达 8
个离散值，表达能力存在天然限制，难以准确拟合实际分布多样的权重参数。为弥补这一不足，3-bit
模型往往引入多个补偿机制，如 per-group scale 和
zero-point，以提升局部的表示精度。在部分模型中（如
<code>q3_K_S</code>​），甚至还叠加了 <strong>双重缩放机制</strong>（scale
× group
scale），这意味着在推理过程中将执行更多的乘法、加法以及额外的访存操作，进一步拖慢执行速度。</p>
<p>更重要的是，3-bit
模型在反量化过程中还面临明显的内存访问瓶颈。由于数据高度压缩、编码结构紧凑，其访存访问呈现出高度跳跃性，不易形成连续读写。这种跳跃性直接影响
CPU
的缓存命中率，进而拉高内存访问延迟，使得即使计算本身不复杂，也因访存效率低而拖慢整体推理过程。在
CPU 资源有限或带宽受限的场景下，这种开销会被进一步放大。</p>
<p>综上所述，3-bit
模型虽然在文件体积和存储压缩方面具备显著优势，但其复杂的反量化计算路径、有限的表达能力补偿机制，以及访存效率低下的结构设计，成为限制其推理性能的主要瓶颈。这一现象也充分说明：<strong>压缩率高并不意味着推理性能强</strong>，特别是在低位宽与复杂解码并存的方案中尤为明显。</p>
<p>相比之下，诸如 <code>q4_0</code>​ 这类的 <strong>4-bit
对称线性量化模型</strong>，虽然压缩比略低，却因其量化结构简单、解码路径短、计算友好度高，通常能在精度可接受的前提下，实现更优的推理速度和整体效率。因此，在实际部署中，<code>q4_0</code>​
这类方案往往成为性能与资源平衡的首选。</p>
<h4 id="neon-simd-向量指令优化">NEON SIMD 向量指令优化</h4>
<p><strong>NEON</strong> 是 ARM 架构下的一种 SIMD（Single Instruction,
Multiple Data）向量指令集扩展，在 ARMv8 架构中，NEON
可实现对多个数据元素的并行处理，例如一次性处理 4 个 <code>float32</code>​
或 8 个 <code>int16</code>​
元素。它广泛应用于矩阵乘法、向量加法、卷积计算等密集型运算场景，尤其适合
LLM 推理所需的大规模张量操作。</p>
<p>在本实验中，Ollama 的底层推理由 <code>llama.cpp</code>​
驱动，该引擎已原生集成了对 NEON 指令的支持。当在 ARM 系统（如鲲鹏
920）上编译 Ollama 时，<code>llama.cpp</code>​ 会自动检测平台架构，并启用
NEON 优化路径，从而在推理过程中显著加速如
<code>matmul</code>​（矩阵乘法）、<code>vector add</code>​（向量加）、<code>fma</code>​（融合乘加）等核心计算流程。</p>
<h5 id="汇编验证流程">汇编验证流程</h5>
<p>为了验证实际运行的 Ollama 二进制是否启用了 NEON
向量指令，我们进行了如下汇编分析流程：</p>
<p><strong>步骤 1：反汇编二进制，导出汇编文件</strong></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">objdump -d /opt/ollama/bin/ollama &gt; ollama.asm</span><br></pre></td></tr></table></figure>
<p>此命令会将 Ollama 可执行文件中的机器码反汇编成汇编指令，输出到
<code>ollama.asm</code>​ 文件中。</p>
<hr>
<p><strong>步骤 2：查找 NEON 指令特征</strong></p>
<p>使用 <code>grep</code>​ 命令筛选常见 NEON SIMD 指令关键字，如
<code>vld1</code>​, <code>vst1</code>​, <code>vadd</code>​,
<code>vmul</code>​, <code>fmla</code>​ 等：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">grep -E <span class="string">"vld1|vst1|vadd|vmul|fmla|ld1|st1|vqdmluh"</span> ollama.asm</span><br></pre></td></tr></table></figure>
<hr>
<p><strong>步骤 3：结果验证示例</strong></p>
<p>部分输出如下：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">101bde8: 4e29cfd5    fmla v21.4s, v30.4s, v9.4s</span><br><span class="line">101bdec: 4e29cd14    fmla v20.4s, v8.4s,  v9.4s</span><br><span class="line">101bdf0: 4e3fcf93    fmla v19.4s, v28.4s, v31.4s</span><br><span class="line">101bdf8: 4e3fcfb2    fmla v18.4s, v29.4s, v31.4s</span><br><span class="line">...</span><br></pre></td></tr></table></figure>
<p>这些指令属于 ARM AArch64 架构下 NEON SIMD 指令集，其中
<code>fmla</code>​ 表示 <strong>融合乘加</strong>（Fused
Multiply-Add）操作，表明 Ollama 的推理计算已被成功编译为 NEON
优化代码路径。</p>
<hr>
<h5 id="汇编语法解释与执行逻辑">汇编语法解释与执行逻辑</h5>
<p>以如下 NEON 指令为例：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">fmla v21.4s, v30.4s, v9.4s</span><br></pre></td></tr></table></figure>
<p>其语义为：执行
<code>v21 = v21 + v30 × v9</code>​，且每个寄存器并行包含 4 个 32
位浮点数，故此条指令等价于对 <strong>4 个 float32
数据同时执行融合乘加操作</strong>，具体解释如下：</p>
<table>
<thead>
<tr>
<th><strong>字段</strong></th>
<th><strong>意义</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td>​<code>fmla</code>​</td>
<td>Fused Multiply-Add（融合乘加）</td>
</tr>
<tr>
<td>​<code>v21.4s</code>​</td>
<td>目标向量寄存器<code>vd</code>​，共 4 个<code>float32</code>​元素</td>
</tr>
<tr>
<td>​<code>v30.4s</code>​</td>
<td>第一个乘数向量<code>vn</code>​</td>
</tr>
<tr>
<td>​<code>v9.4s</code>​</td>
<td>第二个乘数向量<code>vm</code>​</td>
</tr>
</tbody>
</table>
<p>此类操作属于典型的矩阵乘法基本单元，通过 SIMD
实现指令级并行，极大提高了浮点运算密集任务（如注意力机制、MLP 模块）在
CPU 上的执行效率。</p>
<h3 id="模型服务化部署">模型服务化部署</h3>
<p>由于 OpenEuler 20.03 系统中默认的 glibc
版本较低，导致多次尝试通过在线方式安装 Docker
失败。为确保部署成功，我们选择了<strong>离线安装</strong>方式，并对安装流程及
Ollama 模型的 WebUI 部署过程进行了详细记录。</p>
<h4 id="docker-安装配置离线">Docker 安装配置（离线）</h4>
<p><strong>步骤一：配置 Docker 阿里云源</strong></p>
<p>手动创建 <code>.repo</code>​ 文件，写入阿里云镜像源：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cat</span> &gt; /etc/yum.repos.d/docker-ce.repo &lt;&lt;<span class="string">EOF</span></span><br><span class="line"><span class="string">[docker-ce-stable]</span></span><br><span class="line"><span class="string">name=Docker CE Stable</span></span><br><span class="line"><span class="string">baseurl=https://mirrors.aliyun.com/docker-ce/linux/centos/7/aarch64/stable</span></span><br><span class="line"><span class="string">enabled=1</span></span><br><span class="line"><span class="string">gpgcheck=0</span></span><br><span class="line"><span class="string">EOF</span></span><br></pre></td></tr></table></figure>
<p>执行更新缓存：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yum makecache</span><br></pre></td></tr></table></figure>
<hr>
<p><strong>步骤二：安装依赖组件与 SELinux 支持</strong></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">yum install -y \</span><br><span class="line">  device-mapper-persistent-data \</span><br><span class="line">  lvm2 \</span><br><span class="line">  python3-policycoreutils \</span><br><span class="line">  policycoreutils \</span><br><span class="line">  policycoreutils-python-utils \</span><br><span class="line">  selinux-policy \</span><br><span class="line">  selinux-policy-base \</span><br><span class="line">  selinux-policy-targeted</span><br></pre></td></tr></table></figure>
<hr>
<p><strong>步骤三：安装 container-selinux（适用于
OpenEuler）</strong></p>
<p>由于 OpenEuler 默认源中缺失该包，需手动下载并安装：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">wget https://mirrors.huaweicloud.com/centos-altarch/7/extras/aarch64/Packages/container-selinux-2.119.2-1.911c772.el7_8.noarch.rpm</span><br><span class="line">dnf install -y container-selinux-2.119.2-1.911c772.el7_8.noarch.rpm --nogpgcheck</span><br></pre></td></tr></table></figure>
<hr>
<p><strong>步骤四：安装兼容版本的 Docker（推荐 18.06）</strong></p>
<p>注意：较高版本如 23.x 或 28.x 可能因系统 glibc 过旧而无法运行。</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yum install -y docker-ce-18.06.3.ce-3.el7 --nogpgcheck</span><br></pre></td></tr></table></figure>
<hr>
<p><strong>步骤五：启动并设置 Docker 开机自启</strong></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">systemctl start docker</span><br><span class="line">systemctl <span class="built_in">enable</span> docker</span><br></pre></td></tr></table></figure>
<hr>
<p><strong>步骤六：验证安装成功</strong></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">docker --version</span><br><span class="line">docker info</span><br></pre></td></tr></table></figure>
<h4 id="模型-web-服务部署">模型 Web 服务部署</h4>
<p>通过容器部署 Ollama 模型的可视化交互界面，实现远程访问和 Web
浏览器操作。</p>
<hr>
<p><strong>步骤一：查看宿主机 IP 地址</strong></p>
<p>使用命令获取本机局域网地址：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ip a | grep inet</span><br></pre></td></tr></table></figure>
<p>示例输出：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">inet 127.0.0.1/8 scope host lo</span><br><span class="line">inet 192.168.0.223/24 brd 192.168.0.255 scope global dynamic noprefixroute eth0</span><br><span class="line">...</span><br></pre></td></tr></table></figure>
<p>从中可知本机局域网地址为：<code>192.168.0.223</code>​</p>
<hr>
<p><strong>步骤二：运行 WebUI 容器</strong></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">docker run -d -p 3000:8080 \</span><br><span class="line">  --add-host=host.docker.internal:192.168.0.223 \</span><br><span class="line">  -v open-webui:/app/backend/data \</span><br><span class="line">  --name open-webui \</span><br><span class="line">  --restart always \</span><br><span class="line">  ghcr.io/open-webui/open-webui:latest</span><br></pre></td></tr></table></figure>
<p><strong>参数说明：</strong></p>
<table>
<thead>
<tr>
<th>参数</th>
<th>含义说明</th>
</tr>
</thead>
<tbody>
<tr>
<td>​<code>-p 3000:8080</code>​</td>
<td>宿主机端口 3000 映射到容器内的 Web 服务端口 8080</td>
</tr>
<tr>
<td>​<code>--add-host=...</code>​</td>
<td>添加主机名解析，便于容器内访问宿主服务</td>
</tr>
<tr>
<td>​<code>-v open-webui:/...</code>​</td>
<td>设置数据卷，实现 WebUI 持久化存储</td>
</tr>
<tr>
<td>​<code>--restart always</code>​</td>
<td>设置容器异常退出时自动重启</td>
</tr>
<tr>
<td>​<code>ghcr.io/...</code>​</td>
<td>使用官方发布的 WebUI 镜像</td>
</tr>
</tbody>
</table>
<hr>
<p><strong>步骤三（可选）：获取公网 IP（用于远程访问）</strong></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">curl ifconfig.me</span><br></pre></td></tr></table></figure>
<p>输出示例：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">117.78.10.187</span><br></pre></td></tr></table></figure>
<hr>
<p><strong>步骤四：访问 Web 界面</strong></p>
<p>在浏览器中访问：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">http://117.78.10.187:3000</span><br></pre></td></tr></table></figure>
<p>即可打开 Ollama 模型的 Web UI 界面，进行图形化对话与控制操作。</p>
<figure>
<img src="/images/image-20250519160735-qbmy0tv.png" alt="image">
<figcaption aria-hidden="true">image</figcaption>
</figure>
<h2 id="其他小组题目">其他小组题目</h2>
<ol type="1">
<li>图分析算法加速</li>
<li>无损压缩算法性能实验</li>
<li>决策树优化</li>
<li>线性回归算法优化实验</li>
<li>KAE压缩引|擎的数据压缩性能优化实验</li>
<li>FFT算法多层次优化与性能评估</li>
<li>共轭梯度法优化实验</li>
</ol>
<p>‍</p>

            </div>
            
            <div class="post-comments">
                
  <div id="vcomments"></div>
  <!-- 更换为更稳定的 CDN -->
  <script src="https://cdn.jsdelivr.net/npm/valine@1.4.14/dist/Valine.min.js"></script>
  <script>
    new Valine({
      el: '#vcomments',
      appId: 'mxFoM0LRCzAPEPFi5Zy88hIq-gzGzoHsz',
      appKey: 'oJfdGhCSk679QLgYkMEb0lu0',
      placeholder: '有什么想说的嘛...',
      meta: ['nick', 'mail'],
      pageSize: '10',
      visitor: true,
      highlight: true,
      recordIP: false,
      serverURLs: 'https://mxfom0lr.lc-cn-n1-shared.com',
      anonymous: '匿名用户',
      avatar: function() {
        const avatarTypes = ['mp', 'identicon', 'monsterid', 'wavatar', 'retro', 'robohash'];
        return avatarTypes[Math.floor(Math.random() * avatarTypes.length)];
      }(),
      avatarForce: true,
      enableQQ: false,
      beforeComment: function(commentData) {
        if (!commentData.mail) {
          commentData.mail = 'no-email@example.com';
        }
        return commentData;
      },
      avatarCDN: 'https://gravatar.cat.net/avatar/'
    })
  </script>

  <div class="avatar-tips" style="margin-bottom: 15px; color: #666; font-size: 0.9em;">
    <p>请输入一个邮箱再评论。</p>
    <p>如果您在 Gravatar 注册过邮箱，将显示您的 Gravatar 头像；否则将随机显示一个默认头像。</p>
    <p>您可以在 <a href="https://gravatar.com" target="_blank">Gravatar</a> 注册并设置自己的头像。</p>
  </div>

            </div>
        </div>
    </article>
</div>


<nav class="post-nav">
    
    
        <a class="next" href="/2025/05/14/%E4%B8%8D%E5%AE%9A%E5%BC%8F3/">
            不定式#3
            <i class="czs-angle-right-l"></i>
        </a>
    
</nav>



    
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"] ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for(i=0; i < all.length; i += 1) {
            all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
      
      // 添加目录中公式处理的代码
      document.addEventListener('DOMContentLoaded', function() {
        setTimeout(function() {
          if (typeof MathJax !== 'undefined') {
            MathJax.Hub.Queue(["Typeset", MathJax.Hub, document.querySelector('.toc-content')]);
            
            // 为展开/折叠按钮添加MathJax处理
            document.querySelectorAll('.toc-toggle').forEach(function(btn) {
              btn.addEventListener('click', function() {
                setTimeout(function() {
                  MathJax.Hub.Queue(["Typeset", MathJax.Hub, document.querySelector('.toc-content')]);
                }, 100);
              });
            });
          }
        }, 1000);
      });
    </script>
    <script type="text/javascript" src="https://cdn.jsdelivr.net/npm/mathjax@2.7.8/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>



<script src="/js/latest-comments.js"></script>
  
    </main>

    <footer>
        <p>&copy; 2025 1900</p>
    </footer>

    <script>
    document.addEventListener('DOMContentLoaded', () => {
        // 为每个代码块添加复制按钮
        document.querySelectorAll('figure.highlight').forEach(block => {
            const button = document.createElement('button');
            button.className = 'copy-btn';
            button.textContent = '复制';
            block.appendChild(button);

            button.addEventListener('click', () => {
                // 获取代码内容，保留换行符
                const code = block.querySelector('code').innerText;
                
                // 复制到剪贴板
                navigator.clipboard.writeText(code).then(() => {
                    button.textContent = '已复制!';
                    setTimeout(() => {
                        button.textContent = '复制';
                    }, 2000);
                }).catch(err => {
                    console.error('复制失败:', err);
                    button.textContent = '复制失败';
                    setTimeout(() => {
                        button.textContent = '复制';
                    }, 2000);
                });
            });
        });
    });
    </script>
    
    <!-- 确保在布局文件中引入目录脚本 -->
    <script src="/js/toc.js"></script>
</body>
</html>
